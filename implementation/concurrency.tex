\section{Concurrency}
Concurrency was developed lastly as to be able to properly test it with both
Rack applications and static and dynamic files. This section covers the stages
of development the concurrency feature went though, from using threads to
switching to processes.

\subsection{Initial Thread-per-request Implementation}
The initial implementation handled each incoming request in a new thread. This
however, quickly proved to introduce too much overhead in creating a new
threads. Furthermore, as the amount of threads increased, memory use would
rise and eventually bog down the system. It became clear that another
solution was needed, and a system devised of a thread-pool and a job queue was
implemented. 

%thread pool
\subsection{Thread-pool and Job Queue Implementation}
The thread-pool was an array of threads, each running a handler which listened
for requests to be added to the job queue. Each incoming request would be added to
the job queue and then processed by one of the workers (handler in the
    thread-pool). The pros of this
solution was that, with a fixed number of threads, memory use was limited.
Furthermore, the job queue made sure that no requests were dropped if all
threads were busy serving other requests. 

Once implemented, several issues occurred which led to a rethink of the chosen
concurrency model. One problem was that requests would often hang indefinitely
and never return any content to the client. Exactly what caused this bug was
never discovered though many hours were spent trying the track it down. It was
suspected that the threads somehow created a deadlock which in turn would make
the client wait until the browser timed out.  Never discovering the cause of
the bug illustrates one of the key problems of threaded programming; bugs are
often very hard to replicate due to the complexity of the thread model and the
thread scheduler. 

Another problem with the thread-pool implementation was that it performed subpar to the initial implementation of creating a new thread for each request.
This seemed to be caused by the way threads were scheduled, but again it
proved too difficult to pinpoint the exact cause of the poor performance.

Given the difficulty of a thread-based approach, it was decided to implement
the concurrency feature using processes instead.

%process model
\subsection{Process-based Implementation}
Implementing a process-based solution turned out to have many benefits, as it
both reduced the complexity of Yarn, but also greatly improved upon its
performance.

The process-based solution was implemented by using \texttt{Process::fork}
from the Ruby standard library to fork new processes running the server. Each forked
process listens for incoming requests on the socket created in the master
process (the process which forked). Listing~\ref{processimpl} shows the logic for creating new processes.

\bigskip
\begin{lstlisting}[label=processimpl,caption=Process-based implementation
(Yarn/lib/yarn/server.rb:75)]
def init_workers
  @num_workers.times { @workers << fork_worker }
end

def fork_worker
  fork { worker }
end

def worker
  trap("INT") { exit }
  handler = get_handler
  loop do
    @session = @socket.accept
    configure_socket
    handler.run @session
  end
end
\end{lstlisting}

Once the \texttt{fork} method is called on line 6, a new process is started
with a separate Ruby VM and which continues on from that point. Meaning, each
process would invoke the \texttt{worker} method running the loop. On line 10, the
interrupt signal (INT) is trapped, such that when the user presses
\texttt{Ctrl-c}, all processes will exit and Yarn will stop. 

The process-based implementation improved the performance from $\sim$250
requests/second to $\sim$700 requests/second. The performance is covered in
more detail in Section~\ref{eval}. Another improvement of the process-based
concurrency model is that Yarn can now run on in parallel on YARV, whereas the
thread-based implementation would have to be run on JRuby to allow for
parallelism.

